{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "0v_PHCWM0Gz9",
        "outputId": "b43746cd-ee46-458e-8b1f-5ef6ca891c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-1-b1d182f1b24e>:21: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         alpha       beta     m   rho    Y0      price          95cI  \\\n",
              "0          0.2   0.379473  0.34  0.95  0.40   0.177166  2.334980e+01   \n",
              "1          0.2   0.379473  0.34  0.95  0.35   0.203504  2.704929e+01   \n",
              "2          0.2   0.379473  0.34  0.95  0.31   0.225150  2.958679e+01   \n",
              "3          0.2   0.379473  0.34  0.95  0.27   0.247051  3.170565e+01   \n",
              "4          0.2   0.379473  0.34  0.95  0.23   0.269066  3.342225e+01   \n",
              "...        ...        ...   ...   ...   ...        ...           ...   \n",
              "6399875  128.0  22.400000  0.06 -0.55  0.20  78.875664  3.490727e+07   \n",
              "6399876  128.0  22.400000  0.06 -0.55  0.17  78.726395  3.473112e+07   \n",
              "6399877  128.0  22.400000  0.06 -0.55  0.14  78.576637  3.455773e+07   \n",
              "6399878  128.0  22.400000  0.06 -0.55  0.11  78.426895  3.438790e+07   \n",
              "6399879  128.0  22.400000  0.06 -0.55  0.08  78.275841  3.422041e+07   \n",
              "\n",
              "         Maturity  Strike  \n",
              "0             2.0    0.87  \n",
              "1             2.0    0.87  \n",
              "2             2.0    0.87  \n",
              "3             2.0    0.87  \n",
              "4             2.0    0.87  \n",
              "...           ...     ...  \n",
              "6399875       1.0    1.05  \n",
              "6399876       1.0    1.05  \n",
              "6399877       1.0    1.05  \n",
              "6399878       1.0    1.05  \n",
              "6399879       1.0    1.05  \n",
              "\n",
              "[5052128 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f5336fc-11b2-4f2e-bfd2-ee9f37c98429\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alpha</th>\n",
              "      <th>beta</th>\n",
              "      <th>m</th>\n",
              "      <th>rho</th>\n",
              "      <th>Y0</th>\n",
              "      <th>price</th>\n",
              "      <th>95cI</th>\n",
              "      <th>Maturity</th>\n",
              "      <th>Strike</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.379473</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.177166</td>\n",
              "      <td>2.334980e+01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.379473</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.203504</td>\n",
              "      <td>2.704929e+01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.379473</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.225150</td>\n",
              "      <td>2.958679e+01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.379473</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.247051</td>\n",
              "      <td>3.170565e+01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.379473</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.269066</td>\n",
              "      <td>3.342225e+01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6399875</th>\n",
              "      <td>128.0</td>\n",
              "      <td>22.400000</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.20</td>\n",
              "      <td>78.875664</td>\n",
              "      <td>3.490727e+07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6399876</th>\n",
              "      <td>128.0</td>\n",
              "      <td>22.400000</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.17</td>\n",
              "      <td>78.726395</td>\n",
              "      <td>3.473112e+07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6399877</th>\n",
              "      <td>128.0</td>\n",
              "      <td>22.400000</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.14</td>\n",
              "      <td>78.576637</td>\n",
              "      <td>3.455773e+07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6399878</th>\n",
              "      <td>128.0</td>\n",
              "      <td>22.400000</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.11</td>\n",
              "      <td>78.426895</td>\n",
              "      <td>3.438790e+07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6399879</th>\n",
              "      <td>128.0</td>\n",
              "      <td>22.400000</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.08</td>\n",
              "      <td>78.275841</td>\n",
              "      <td>3.422041e+07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5052128 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f5336fc-11b2-4f2e-bfd2-ee9f37c98429')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f5336fc-11b2-4f2e-bfd2-ee9f37c98429 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f5336fc-11b2-4f2e-bfd2-ee9f37c98429');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cc138e1f-1ac0-4599-8fcc-1818b86a29d0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc138e1f-1ac0-4599-8fcc-1818b86a29d0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cc138e1f-1ac0-4599-8fcc-1818b86a29d0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to your CSV files directory\n",
        "csv_directory = \"/content/\"\n",
        "\n",
        "# Get a list of all CSV files in the directory\n",
        "csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
        "\n",
        "# Function to extract maturity and strike from filename\n",
        "def extract_info(filename):\n",
        "    base_name = os.path.splitext(os.path.basename(filename))[0]\n",
        "    parts = base_name.split('Tmt')[1].split('Str')\n",
        "    maturity = float(parts[0])\n",
        "    strike = float(parts[1])\n",
        "    return maturity, strike\n",
        "\n",
        "# Function to process CSV file\n",
        "def process_csv(file_path):\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Extract maturity and strike from filename\n",
        "    maturity, strike = extract_info(file_path)\n",
        "    # Add maturity and strike as columns\n",
        "    df['Maturity'] = maturity\n",
        "    df['Strike'] = strike\n",
        "    return df\n",
        "\n",
        "# Process each CSV file and concatenate\n",
        "all_dataframes = []\n",
        "for file_name in csv_files:\n",
        "    file_path = os.path.join(csv_directory, file_name)\n",
        "    df = process_csv(file_path)\n",
        "    all_dataframes.append(df)\n",
        "\n",
        "# Concatenate all datasets\n",
        "concatenated_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "# Drop rows containing 'inf' values in any column\n",
        "df = concatenated_df[~concatenated_df.isin([' inf']).any(axis=1)].reset_index(drop=True)\n",
        "df = concatenated_df\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.astype('float32')\n",
        "df = df[df['price']<100]\n",
        "# Print the DataFrame after dropping rows\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data\n",
        "X = df[['alpha', 'beta', 'm', 'rho', 'Y0', 'Maturity', 'Strike']].values\n",
        "y = df['price'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define RMSE loss function\n",
        "def rmse_loss(y_true, y_pred):\n",
        "    return torch.sqrt(torch.mean((y_true - y_pred)**2))\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32768\n",
        "\n",
        "# Standardize the data\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()  # Scale the target variable\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()  # Scale the target variable\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create DataLoader for training and testing sets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(7, 256)  # Increase neurons\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # BatchNorm layer\n",
        "        self.fc2 = nn.Linear(256, 512)  # Additional layer\n",
        "        self.bn2 = nn.BatchNorm1d(512)  # BatchNorm layer\n",
        "        self.fc3 = nn.Linear(512, 1024)   # Additional layer\n",
        "        self.bn3 = nn.BatchNorm1d(1024)  # BatchNorm layer\n",
        "        self.fc4 = nn.Linear(1024, 2048)   # Additional layer\n",
        "        self.bn4 = nn.BatchNorm1d(2048)  # BatchNorm layer\n",
        "        self.fc5 = nn.Linear(2048, 1024)   # Additional layer\n",
        "        self.bn5 = nn.BatchNorm1d(1024)  # BatchNorm layer\n",
        "        self.fc6 = nn.Linear(1024, 512)   # Additional layer\n",
        "        self.bn6 = nn.BatchNorm1d(512)  # BatchNorm layer\n",
        "        self.fc7 = nn.Linear(512, 256)   # Additional layer\n",
        "        self.bn7 = nn.BatchNorm1d(256)  # BatchNorm layer\n",
        "        self.fc8 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.dropout3 = nn.Dropout(0.6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(self.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.dropout2(self.relu(self.bn3(self.fc3(x))))\n",
        "        x = self.dropout3(self.relu(self.bn4(self.fc4(x))))\n",
        "        x = self.dropout3(self.relu(self.bn5(self.fc5(x))))\n",
        "        x = self.dropout2(self.relu(self.bn6(self.fc6(x))))\n",
        "        x = self.dropout1(self.relu(self.bn7(self.fc7(x))))\n",
        "        x = self.fc8(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to GPU\n",
        "model = Net().cuda()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = rmse_loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.1, verbose=True)\n",
        "\n",
        "# Define early stopping parameters\n",
        "patience = 20\n",
        "best_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Warm-up phase\n",
        "warmup_epochs = 2\n",
        "for epoch in range(warmup_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 150\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        running_val_loss = 0.0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            running_val_loss += val_loss.item() * inputs.size(0)\n",
        "        epoch_val_loss = running_val_loss / len(test_dataset)\n",
        "\n",
        "    scheduler.step(epoch_val_loss)\n",
        "\n",
        "    if epoch_val_loss < best_loss:\n",
        "        best_loss = epoch_val_loss\n",
        "        counter = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    if (epoch+1) % 1 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = model(inputs)\n",
        "        # Unscale predictions\n",
        "        unscaled_outputs = scaler_y.inverse_transform(outputs.cpu().numpy()).flatten()\n",
        "        unscaled_labels = scaler_y.inverse_transform(labels.cpu().numpy()).flatten()\n",
        "        # Calculate RMSE loss on unscaled predictions and labels\n",
        "        unscaled_loss = criterion(torch.tensor(unscaled_outputs), torch.tensor(unscaled_labels))\n",
        "        running_loss += unscaled_loss.item() * inputs.size(0)\n",
        "    unscaled_test_loss = running_loss / len(test_dataset)\n",
        "    print(f'Unscaled Test Loss: {unscaled_test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32AneczTEf2I",
        "outputId": "ae9fdb47-079a-4b70-85fa-1a68ce767c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/150], Train Loss: 0.9023, Val Loss: 0.8932\n",
            "Epoch [2/150], Train Loss: 0.8987, Val Loss: 0.8888\n",
            "Epoch [3/150], Train Loss: 0.8938, Val Loss: 0.8869\n",
            "Epoch [4/150], Train Loss: 0.8900, Val Loss: 0.8802\n",
            "Epoch [5/150], Train Loss: 0.8856, Val Loss: 0.8724\n",
            "Epoch [6/150], Train Loss: 0.8803, Val Loss: 0.8749\n",
            "Epoch [7/150], Train Loss: 0.8756, Val Loss: 0.8617\n",
            "Epoch [8/150], Train Loss: 0.8689, Val Loss: 0.8546\n",
            "Epoch [9/150], Train Loss: 0.8646, Val Loss: 0.8412\n",
            "Epoch [10/150], Train Loss: 0.8580, Val Loss: 0.8411\n",
            "Epoch [11/150], Train Loss: 0.8531, Val Loss: 0.8350\n",
            "Epoch [12/150], Train Loss: 0.8473, Val Loss: 0.8292\n",
            "Epoch [13/150], Train Loss: 0.8410, Val Loss: 0.8208\n",
            "Epoch [14/150], Train Loss: 0.8340, Val Loss: 0.8112\n",
            "Epoch [15/150], Train Loss: 0.8277, Val Loss: 0.7997\n",
            "Epoch [16/150], Train Loss: 0.8230, Val Loss: 0.7985\n",
            "Epoch [17/150], Train Loss: 0.8208, Val Loss: 0.8062\n",
            "Epoch [18/150], Train Loss: 0.8139, Val Loss: 0.7866\n",
            "Epoch [19/150], Train Loss: 0.8081, Val Loss: 0.7902\n",
            "Epoch [20/150], Train Loss: 0.8055, Val Loss: 0.7811\n",
            "Epoch [21/150], Train Loss: 0.8026, Val Loss: 0.7766\n",
            "Epoch [22/150], Train Loss: 0.7970, Val Loss: 0.7724\n",
            "Epoch [23/150], Train Loss: 0.7965, Val Loss: 0.7732\n",
            "Epoch [24/150], Train Loss: 0.7930, Val Loss: 0.7624\n",
            "Epoch [25/150], Train Loss: 0.7882, Val Loss: 0.7550\n",
            "Epoch [26/150], Train Loss: 0.7850, Val Loss: 0.7617\n",
            "Epoch [27/150], Train Loss: 0.7822, Val Loss: 0.7533\n",
            "Epoch [28/150], Train Loss: 0.7796, Val Loss: 0.7575\n",
            "Epoch [29/150], Train Loss: 0.7780, Val Loss: 0.7544\n",
            "Epoch [30/150], Train Loss: 0.7747, Val Loss: 0.7470\n",
            "Epoch [31/150], Train Loss: 0.7736, Val Loss: 0.7557\n",
            "Epoch [32/150], Train Loss: 0.7714, Val Loss: 0.7417\n",
            "Epoch [33/150], Train Loss: 0.7686, Val Loss: 0.7435\n",
            "Epoch [34/150], Train Loss: 0.7669, Val Loss: 0.7365\n",
            "Epoch [35/150], Train Loss: 0.7636, Val Loss: 0.7339\n",
            "Epoch [36/150], Train Loss: 0.7630, Val Loss: 0.7330\n",
            "Epoch [37/150], Train Loss: 0.7615, Val Loss: 0.7331\n",
            "Epoch [38/150], Train Loss: 0.7600, Val Loss: 0.7315\n",
            "Epoch [39/150], Train Loss: 0.7579, Val Loss: 0.7326\n",
            "Epoch [40/150], Train Loss: 0.7577, Val Loss: 0.7301\n",
            "Epoch [41/150], Train Loss: 0.7553, Val Loss: 0.7296\n",
            "Epoch [42/150], Train Loss: 0.7546, Val Loss: 0.7255\n",
            "Epoch [43/150], Train Loss: 0.7546, Val Loss: 0.7255\n",
            "Epoch [44/150], Train Loss: 0.7534, Val Loss: 0.7290\n",
            "Epoch [45/150], Train Loss: 0.7509, Val Loss: 0.7262\n",
            "Epoch [46/150], Train Loss: 0.7492, Val Loss: 0.7349\n",
            "Epoch [47/150], Train Loss: 0.7508, Val Loss: 0.7297\n",
            "Epoch [48/150], Train Loss: 0.7475, Val Loss: 0.7248\n",
            "Epoch [49/150], Train Loss: 0.7479, Val Loss: 0.7237\n",
            "Epoch [50/150], Train Loss: 0.7478, Val Loss: 0.7217\n",
            "Epoch [51/150], Train Loss: 0.7452, Val Loss: 0.7239\n",
            "Epoch [52/150], Train Loss: 0.7454, Val Loss: 0.7228\n",
            "Epoch [53/150], Train Loss: 0.7444, Val Loss: 0.7224\n",
            "Epoch [54/150], Train Loss: 0.7440, Val Loss: 0.7222\n",
            "Epoch [55/150], Train Loss: 0.7440, Val Loss: 0.7214\n",
            "Epoch [56/150], Train Loss: 0.7413, Val Loss: 0.7177\n",
            "Epoch [57/150], Train Loss: 0.7406, Val Loss: 0.7196\n",
            "Epoch [58/150], Train Loss: 0.7411, Val Loss: 0.7206\n",
            "Epoch [59/150], Train Loss: 0.7395, Val Loss: 0.7202\n",
            "Epoch [60/150], Train Loss: 0.7389, Val Loss: 0.7175\n",
            "Epoch [61/150], Train Loss: 0.7382, Val Loss: 0.7171\n",
            "Epoch [62/150], Train Loss: 0.7391, Val Loss: 0.7174\n",
            "Epoch [63/150], Train Loss: 0.7356, Val Loss: 0.7156\n",
            "Epoch [64/150], Train Loss: 0.7355, Val Loss: 0.7232\n",
            "Epoch [65/150], Train Loss: 0.7390, Val Loss: 0.7156\n",
            "Epoch [66/150], Train Loss: 0.7348, Val Loss: 0.7136\n",
            "Epoch [67/150], Train Loss: 0.7358, Val Loss: 0.7133\n",
            "Epoch [68/150], Train Loss: 0.7333, Val Loss: 0.7128\n",
            "Epoch [69/150], Train Loss: 0.7345, Val Loss: 0.7144\n",
            "Epoch [70/150], Train Loss: 0.7346, Val Loss: 0.7106\n",
            "Epoch [71/150], Train Loss: 0.7321, Val Loss: 0.7109\n",
            "Epoch [72/150], Train Loss: 0.7311, Val Loss: 0.7117\n",
            "Epoch [73/150], Train Loss: 0.7323, Val Loss: 0.7135\n",
            "Epoch [74/150], Train Loss: 0.7323, Val Loss: 0.7109\n",
            "Epoch [75/150], Train Loss: 0.7316, Val Loss: 0.7108\n",
            "Epoch [76/150], Train Loss: 0.7306, Val Loss: 0.7104\n",
            "Epoch [77/150], Train Loss: 0.7287, Val Loss: 0.7092\n",
            "Epoch [78/150], Train Loss: 0.7288, Val Loss: 0.7100\n",
            "Epoch [79/150], Train Loss: 0.7270, Val Loss: 0.7355\n",
            "Epoch [80/150], Train Loss: 0.7303, Val Loss: 0.7119\n",
            "Epoch [81/150], Train Loss: 0.7289, Val Loss: 0.7079\n",
            "Epoch [82/150], Train Loss: 0.7267, Val Loss: 0.7108\n",
            "Epoch [83/150], Train Loss: 0.7289, Val Loss: 0.7121\n",
            "Epoch [84/150], Train Loss: 0.7267, Val Loss: 0.7098\n",
            "Epoch [85/150], Train Loss: 0.7248, Val Loss: 0.7061\n",
            "Epoch [86/150], Train Loss: 0.7249, Val Loss: 0.7065\n",
            "Epoch [87/150], Train Loss: 0.7261, Val Loss: 0.7075\n",
            "Epoch [88/150], Train Loss: 0.7250, Val Loss: 0.7139\n",
            "Epoch [89/150], Train Loss: 0.7263, Val Loss: 0.7091\n",
            "Epoch [90/150], Train Loss: 0.7242, Val Loss: 0.7072\n",
            "Epoch [91/150], Train Loss: 0.7250, Val Loss: 0.7085\n",
            "Epoch [92/150], Train Loss: 0.7250, Val Loss: 0.7050\n",
            "Epoch [93/150], Train Loss: 0.7235, Val Loss: 0.7047\n",
            "Epoch [94/150], Train Loss: 0.7224, Val Loss: 0.7066\n",
            "Epoch [95/150], Train Loss: 0.7238, Val Loss: 0.7071\n",
            "Epoch [96/150], Train Loss: 0.7244, Val Loss: 0.7072\n",
            "Epoch [97/150], Train Loss: 0.7228, Val Loss: 0.7061\n",
            "Epoch [98/150], Train Loss: 0.7230, Val Loss: 0.7073\n",
            "Epoch [99/150], Train Loss: 0.7206, Val Loss: 0.7049\n",
            "Epoch [100/150], Train Loss: 0.7225, Val Loss: 0.7060\n",
            "Epoch [101/150], Train Loss: 0.7213, Val Loss: 0.7049\n",
            "Epoch [102/150], Train Loss: 0.7230, Val Loss: 0.7036\n",
            "Epoch [103/150], Train Loss: 0.7195, Val Loss: 0.7061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(range(len(unscaled_outputs)),unscaled_outputs,label='Predictions')\n",
        "plt.scatter(range(len(unscaled_labels)),unscaled_labels,label='Truth')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "6OrzrvtlraQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}